{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014bb2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10113 entries, 0 to 10112\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   symbol              10113 non-null  object \n",
      " 1   statement_type      10113 non-null  object \n",
      " 2   period              10113 non-null  object \n",
      " 3   period_detail       10113 non-null  object \n",
      " 4   period_end_date     10113 non-null  object \n",
      " 5   report_type         10113 non-null  object \n",
      " 6   consolidation_type  10113 non-null  object \n",
      " 7   status              1113 non-null   float64\n",
      " 8   s3_path             8844 non-null   object \n",
      " 9   pdf_folder_path     10113 non-null  object \n",
      " 10  statement_ID        10113 non-null  object \n",
      " 11  pdf_s3_path         9952 non-null   object \n",
      " 12  is_missing          10113 non-null  bool   \n",
      " 13  is_mismatched       10113 non-null  bool   \n",
      " 14  is_incomplete       10113 non-null  bool   \n",
      " 15  is_incomplete_note  1126 non-null   object \n",
      " 16  is_mismatched_note  0 non-null      float64\n",
      "dtypes: bool(3), float64(2), object(12)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fin_stat_meta_csv = \"financial_statements_metadata_3_09_2025_with_ID - financial_statements_metadata_3_09_2025 - financial_statements_metadata_3_09_2025 (1).csv\"\n",
    "fin_stat_meta_df = pd.read_csv(fin_stat_meta_csv)\n",
    "fin_stat_meta_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3536a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of duplicate s3_paths is 488\n",
      "status\n",
      "1.0    16\n",
      "dtype: int64\n",
      "statement_type\n",
      "audited      488\n",
      "unaudited    488\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First, identify which s3_paths are duplicates (appear more than once)\n",
    "duplicate_s3_paths = fin_stat_meta_df['s3_path'].value_counts()\n",
    "duplicate_s3_paths = duplicate_s3_paths[duplicate_s3_paths > 1].index\n",
    "print(f\"The number of duplicate s3_paths is {len(duplicate_s3_paths)}\")\n",
    "\n",
    "# Filter the dataframe to only rows with duplicate s3_paths\n",
    "df_duplicates = fin_stat_meta_df[fin_stat_meta_df['s3_path'].isin(duplicate_s3_paths)]\n",
    "\n",
    "# Count how many duplicate rows exist per status\n",
    "duplicate_count_by_status = df_duplicates.groupby('status').size()\n",
    "print(duplicate_count_by_status)\n",
    "df_duplicates.to_csv(\"df_duplicates.csv\", index=False)\n",
    "\n",
    "# Count how many duplicate rows exist per statement_type\n",
    "duplicate_count_by_statement_type = df_duplicates.groupby('statement_type').size()\n",
    "print(duplicate_count_by_statement_type)\n",
    "\n",
    "# Count how many duplicate rows exist per report_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd36df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 10113\n",
      "Cleaned rows: 9153\n",
      "Rows dropped: 960\n",
      "\n",
      "Remaining duplicate s3_paths: 8\n",
      "Note: Some s3_paths still have duplicates - checking their status:\n",
      "  s3://jse-renamed-docs-copy/CSV-Copy/BIL/audited_financial_statements/2024/bil-barita_investments_limited_consolidated_statement_of_comprehensive_income-30-september-2024.csv: status values = [1.]\n",
      "  s3://jse-renamed-docs-copy/CSV-Copy/MFS/unaudited_financial_statements/2023/mfs-MFS_capital_partners_limited_unaudited_consolidated_statement_of_financial_position-june-30-2023.csv: status values = [1.]\n",
      "  s3://jse-renamed-docs-copy/CSV-Copy/CAC/audited_financial_statements/2022/cac-cac_2000_limited_statement_of_comprehensive_income-31-october-2022.csv: status values = [1.]\n",
      "  s3://jse-renamed-docs-copy/CSV-Copy/KREMI/unaudited_financial_statements/2020/kremi-caribbean_cream_ltd_unaudited_income_statement-august-31-2020.csv: status values = [1.]\n",
      "  s3://jse-renamed-docs-copy/CSV-Copy/BIL/unaudited_financial_statements/2020/bil-bartita_investments_limited_consolidated_statement_of_cash_flows-june-30-2020.csv: status values = [1.]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify which s3_paths are duplicated\n",
    "duplicate_s3_paths = fin_stat_meta_df['s3_path'].value_counts()\n",
    "duplicate_s3_paths = duplicate_s3_paths[duplicate_s3_paths > 1].index\n",
    "\n",
    "# Step 2: Create a boolean mask for rows to keep\n",
    "# Keep a row if:\n",
    "# - Its s3_path is NOT duplicated (keep regardless of status), OR\n",
    "# - Its s3_path IS duplicated AND status = 1.0\n",
    "mask_keep = (\n",
    "    ~fin_stat_meta_df['s3_path'].isin(duplicate_s3_paths) |  # Not duplicated\n",
    "    ((fin_stat_meta_df['s3_path'].isin(duplicate_s3_paths)) & (fin_stat_meta_df['status'] == 1.0))  # Duplicated but status = 1\n",
    ")\n",
    "\n",
    "# Step 3: Apply the mask to keep only the desired rows\n",
    "fin_stat_meta_df_cleaned = fin_stat_meta_df[mask_keep].copy()\n",
    "\n",
    "# Step 4: Check the results\n",
    "print(f\"Original rows: {len(fin_stat_meta_df)}\")\n",
    "print(f\"Cleaned rows: {len(fin_stat_meta_df_cleaned)}\")\n",
    "print(f\"Rows dropped: {len(fin_stat_meta_df) - len(fin_stat_meta_df_cleaned)}\")\n",
    "\n",
    "# Verify no duplicates remain (or only status=1.0 duplicates if they exist)\n",
    "remaining_duplicates = fin_stat_meta_df_cleaned['s3_path'].value_counts()\n",
    "remaining_duplicates = remaining_duplicates[remaining_duplicates > 1]\n",
    "print(f\"\\nRemaining duplicate s3_paths: {len(remaining_duplicates)}\")\n",
    "if len(remaining_duplicates) > 0:\n",
    "    print(\"Note: Some s3_paths still have duplicates - checking their status:\")\n",
    "    for s3_path in remaining_duplicates.index[:5]:  # Show first 5\n",
    "        statuses = fin_stat_meta_df_cleaned[fin_stat_meta_df_cleaned['s3_path'] == s3_path]['status'].unique()\n",
    "        print(f\"  {s3_path}: status values = {statuses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e8c7d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows with remaining duplicate s3_paths: 16\n",
      "\n",
      "Duplicate rows with status=1.0:\n",
      "      symbol                                                                                                                                                                               s3_path  status statement_type     period period_end_date       report_type\n",
      "9064     BIL         s3://jse-renamed-docs-copy/CSV-Copy/BIL/audited_financial_statements/2022/bil-barita_investments_limited_consolidated_statement_of_comprehensive_income-30-september-2022.csv     1.0        audited     annual       30/9/2022     balance_sheet\n",
      "9065     BIL         s3://jse-renamed-docs-copy/CSV-Copy/BIL/audited_financial_statements/2022/bil-barita_investments_limited_consolidated_statement_of_comprehensive_income-30-september-2022.csv     1.0        audited     annual       30/9/2022          cashflow\n",
      "9069     BIL         s3://jse-renamed-docs-copy/CSV-Copy/BIL/audited_financial_statements/2024/bil-barita_investments_limited_consolidated_statement_of_comprehensive_income-30-september-2024.csv     1.0        audited     annual       30/9/2024  income_statement\n",
      "9070     BIL         s3://jse-renamed-docs-copy/CSV-Copy/BIL/audited_financial_statements/2024/bil-barita_investments_limited_consolidated_statement_of_comprehensive_income-30-september-2024.csv     1.0        audited     annual       30/9/2024     balance_sheet\n",
      "9537     BIL                     s3://jse-renamed-docs-copy/CSV-Copy/BIL/unaudited_financial_statements/2020/bil-bartita_investments_limited_consolidated_statement_of_cash_flows-june-30-2020.csv     1.0      unaudited  quarterly       30/6/2020          cashflow\n",
      "9540     BIL                     s3://jse-renamed-docs-copy/CSV-Copy/BIL/unaudited_financial_statements/2020/bil-bartita_investments_limited_consolidated_statement_of_cash_flows-june-30-2020.csv     1.0      unaudited  quarterly       30/6/2020          cashflow\n",
      "9138     CAC                                  s3://jse-renamed-docs-copy/CSV-Copy/CAC/audited_financial_statements/2022/cac-cac_2000_limited_statement_of_comprehensive_income-31-october-2022.csv     1.0        audited     annual      31/10/2022  income_statement\n",
      "9147     CAC                                  s3://jse-renamed-docs-copy/CSV-Copy/CAC/audited_financial_statements/2022/cac-cac_2000_limited_statement_of_comprehensive_income-31-october-2022.csv     1.0        audited     annual      31/10/2022  income_statement\n",
      "9319     DTL           s3://jse-renamed-docs-copy/CSV-Copy/DTL/audited_financial_statements/2018/dtl-derrimon_trading_company_limited_group_statement_of_comprehensive_income-31-december-2018.csv     1.0        audited     annual      31/12/2018  income_statement\n",
      "9322     DTL           s3://jse-renamed-docs-copy/CSV-Copy/DTL/audited_financial_statements/2018/dtl-derrimon_trading_company_limited_group_statement_of_comprehensive_income-31-december-2018.csv     1.0        audited     annual      31/12/2018  income_statement\n",
      "9958     JBG            s3://jse-renamed-docs-copy/CSV-Copy/JBG/unaudited_financial_statements/2023/jbg-jamaica_broilers_group_limited_group_statement_of_comprehensive_income-october-28-2023.csv     1.0      unaudited  quarterly      28/10/2023  income_statement\n",
      "9959     JBG            s3://jse-renamed-docs-copy/CSV-Copy/JBG/unaudited_financial_statements/2023/jbg-jamaica_broilers_group_limited_group_statement_of_comprehensive_income-october-28-2023.csv     1.0      unaudited  quarterly      28/10/2023  income_statement\n",
      "9651   KREMI                                 s3://jse-renamed-docs-copy/CSV-Copy/KREMI/unaudited_financial_statements/2020/kremi-caribbean_cream_ltd_unaudited_income_statement-august-31-2020.csv     1.0      unaudited  quarterly       31/8/2020  income_statement\n",
      "9652   KREMI                                 s3://jse-renamed-docs-copy/CSV-Copy/KREMI/unaudited_financial_statements/2020/kremi-caribbean_cream_ltd_unaudited_income_statement-august-31-2020.csv     1.0      unaudited  quarterly       31/8/2020  income_statement\n",
      "10094    MFS  s3://jse-renamed-docs-copy/CSV-Copy/MFS/unaudited_financial_statements/2023/mfs-MFS_capital_partners_limited_unaudited_consolidated_statement_of_financial_position-june-30-2023.csv     1.0      unaudited  quarterly       30/6/2023     balance_sheet\n",
      "10095    MFS  s3://jse-renamed-docs-copy/CSV-Copy/MFS/unaudited_financial_statements/2023/mfs-MFS_capital_partners_limited_unaudited_consolidated_statement_of_financial_position-june-30-2023.csv     1.0      unaudited  quarterly       30/6/2023     balance_sheet\n"
     ]
    }
   ],
   "source": [
    "# Let's examine these remaining duplicates to see what differs between them\n",
    "remaining_duplicate_paths = fin_stat_meta_df_cleaned['s3_path'].value_counts()\n",
    "remaining_duplicate_paths = remaining_duplicate_paths[remaining_duplicate_paths > 1].index\n",
    "\n",
    "print(f\"Total rows with remaining duplicate s3_paths: {fin_stat_meta_df_cleaned['s3_path'].isin(remaining_duplicate_paths).sum()}\")\n",
    "\n",
    "# Show all the duplicate rows to see what's different\n",
    "duplicate_rows = fin_stat_meta_df_cleaned[fin_stat_meta_df_cleaned['s3_path'].isin(remaining_duplicate_paths)]\n",
    "duplicate_rows_sorted = duplicate_rows.sort_values('s3_path')\n",
    "duplicate_rows_sorted.to_csv(\"remaining_duplicates.csv\", index=False)\n",
    "\n",
    "# Display key columns to understand the differences\n",
    "print(\"\\nDuplicate rows with status=1.0:\")\n",
    "print(duplicate_rows_sorted[['symbol', 's3_path', 'status', 'statement_type', 'period', 'period_end_date', 'report_type']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47a51a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate s3_paths to review: 8\n",
      "Mapped so far: 8\n",
      "Still need to review: 0\n",
      "\n",
      "Rows before: 9153\n",
      "Rows after: 9137\n",
      "Rows dropped: 16\n",
      "\n",
      "Remaining duplicate s3_paths: 0\n"
     ]
    }
   ],
   "source": [
    "# Manual mapping: s3_path -> correct statement_type to keep\n",
    "# Review each file and update this mapping with the correct statement type\n",
    "correct_statement_mapping = {\n",
    "    # BIL files\n",
    "    's3://jse-renamed-docs-copy/CSV-Copy/BIL/audited_financial_statements/2022/bil-barita_investments_limited_consolidated_statement_of_comprehensive_income-30-september-2022.csv': 'cashflow',  # Verified: contains cashflow\n",
    "    's3://jse-renamed-docs-copy/CSV-Copy/BIL/audited_financial_statements/2024/bil-barita_investments_limited_consolidated_statement_of_comprehensive_income-30-september-2024.csv': 'income_statement',  # Verified: contains income statement\n",
    "    's3://jse-renamed-docs-copy/CSV-Copy/BIL/unaudited_financial_statements/2020/bil-bartita_investments_limited_consolidated_statement_of_cash_flows-june-30-2020.csv': 'cashflow',  # TODO: verify\n",
    "    \n",
    "    # Other companies - TODO: review these files\n",
    "    's3://jse-renamed-docs-copy/CSV-Copy/CAC/audited_financial_statements/2022/cac-cac_2000_limited_statement_of_comprehensive_income-31-october-2022.csv': 'income_statement',  # TODO: verify\n",
    "    's3://jse-renamed-docs-copy/CSV-Copy/DTL/audited_financial_statements/2018/dtl-derrimon_trading_company_limited_group_statement_of_comprehensive_income-31-december-2018.csv': 'income_statement',  # TODO: verify\n",
    "    's3://jse-renamed-docs-copy/CSV-Copy/JBG/unaudited_financial_statements/2023/jbg-jamaica_broilers_group_limited_group_statement_of_comprehensive_income-october-28-2023.csv': 'income_statement',  # TODO: verify\n",
    "    's3://jse-renamed-docs-copy/CSV-Copy/KREMI/unaudited_financial_statements/2020/kremi-caribbean_cream_ltd_unaudited_income_statement-august-31-2020.csv': 'income_statement',  # TODO: verify\n",
    "    's3://jse-renamed-docs-copy/CSV-Copy/MFS/unaudited_financial_statements/2023/mfs-MFS_capital_partners_limited_unaudited_consolidated_statement_of_financial_position-june-30-2023.csv': 'balance_sheet',  # TODO: verify\n",
    "}\n",
    "\n",
    "# Get list of duplicate s3_paths for reference\n",
    "remaining_duplicate_paths = fin_stat_meta_df_cleaned['s3_path'].value_counts()\n",
    "remaining_duplicate_paths = remaining_duplicate_paths[remaining_duplicate_paths > 1].index\n",
    "\n",
    "print(f\"Total duplicate s3_paths to review: {len(remaining_duplicate_paths)}\")\n",
    "print(f\"Mapped so far: {len(correct_statement_mapping)}\")\n",
    "print(f\"Still need to review: {len(remaining_duplicate_paths) - len(correct_statement_mapping)}\")\n",
    "\n",
    "# Apply the mapping\n",
    "def should_keep_row(row):\n",
    "    s3_path = row['s3_path']\n",
    "    statement_type = row['statement_type']\n",
    "    \n",
    "    # If this s3_path is in our mapping, only keep if it matches the correct type\n",
    "    if s3_path in correct_statement_mapping:\n",
    "        return statement_type == correct_statement_mapping[s3_path]\n",
    "    else:\n",
    "        # Not in mapping (not a duplicate issue), keep it\n",
    "        return True\n",
    "\n",
    "# Apply the filter\n",
    "mask = fin_stat_meta_df_cleaned.apply(should_keep_row, axis=1)\n",
    "fin_stat_meta_df_final = fin_stat_meta_df_cleaned[mask].copy()\n",
    "\n",
    "print(f\"\\nRows before: {len(fin_stat_meta_df_cleaned)}\")\n",
    "print(f\"Rows after: {len(fin_stat_meta_df_final)}\")\n",
    "print(f\"Rows dropped: {len(fin_stat_meta_df_cleaned) - len(fin_stat_meta_df_final)}\")\n",
    "\n",
    "# Verify no duplicates remain\n",
    "remaining = fin_stat_meta_df_final['s3_path'].value_counts()\n",
    "remaining_dups = remaining[remaining > 1]\n",
    "print(f\"\\nRemaining duplicate s3_paths: {len(remaining_dups)}\")\n",
    "\n",
    "if len(remaining_dups) > 0:\n",
    "    print(\"\\nWARNING: Still have duplicates - need to add these to mapping:\")\n",
    "    for s3_path in remaining_dups.index:\n",
    "        print(f\"  - {s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72bbbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, identify which s3_paths are duplicates (appear more than once)\n",
    "duplicate_s3_paths = fin_stat_meta_df['s3_path'].value_counts()\n",
    "duplicate_s3_paths = duplicate_s3_paths[duplicate_s3_paths > 1].index\n",
    "print(f\"The number of duplicate s3_paths is {len(duplicate_s3_paths)}\")\n",
    "\n",
    "# Filter the dataframe to only rows with duplicate s3_paths\n",
    "df_duplicates = fin_stat_meta_df[fin_stat_meta_df['s3_path'].isin(duplicate_s3_paths)]\n",
    "\n",
    "# Count how many duplicate rows exist per status\n",
    "duplicate_count_by_status = df_duplicates.groupby('status').size()\n",
    "print(duplicate_count_by_status)\n",
    "df_duplicates.to_csv(\"df_duplicates.csv\", index=False)\n",
    "\n",
    "# Count how many duplicate rows exist per statement_type\n",
    "duplicate_count_by_statement_type = df_duplicates.groupby('statement_type').size()\n",
    "print(duplicate_count_by_statement_type)\n",
    "\n",
    "# Count how many duplicate rows exist per report_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00282b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8508 entries, 7 to 7794\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   symbol              8508 non-null   object \n",
      " 1   statement_type      8508 non-null   object \n",
      " 2   period              8508 non-null   object \n",
      " 3   period_detail       8508 non-null   object \n",
      " 4   period_end_date     8508 non-null   object \n",
      " 5   report_type         8508 non-null   object \n",
      " 6   consolidation_type  8508 non-null   object \n",
      " 7   status              1072 non-null   float64\n",
      " 8   s3_path             7450 non-null   object \n",
      " 9   pdf_folder_path     8508 non-null   object \n",
      " 10  statement_ID        8508 non-null   object \n",
      " 11  pdf_s3_path         8348 non-null   object \n",
      " 12  is_missing          8508 non-null   bool   \n",
      " 13  is_mismatched       8508 non-null   bool   \n",
      " 14  is_incomplete       8508 non-null   bool   \n",
      " 15  is_incomplete_note  859 non-null    object \n",
      " 16  is_mismatched_note  0 non-null      float64\n",
      "dtypes: bool(3), float64(2), object(12)\n",
      "memory usage: 1022.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Sort by the grouping columns and status (putting status=1 first)\n",
    "# Then drop duplicates keeping the first occurrence (which will be status=1 if it exists)\n",
    "fin_stat_meta_df_cleaned = (fin_stat_meta_df\n",
    "    .sort_values(['symbol', 'period_end_date', 'report_type', 'consolidation_type', 'status'], \n",
    "                 na_position='last')  # This puts blank/NaN values last\n",
    "    .drop_duplicates(subset=['symbol', 'period_end_date', 'report_type', 'consolidation_type'], \n",
    "                     keep='first')\n",
    ")\n",
    "fin_stat_meta_df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718406d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = fin_stat_meta_df[\"s3_path\"].value_counts()\n",
    "duplicates = counts[counts > 1]\n",
    "print(duplicates)\n",
    "# Add a running count column to show the order of appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value counts and filter for counts > 1\n",
    "counts = fin_stat_meta_df[[\"report_type\", \"statement_ID\"]].value_counts()\n",
    "duplicates = counts[counts > 1]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a running count column to show the order of appearance\n",
    "fin_stat_meta_df['combination_order'] = fin_stat_meta_df.groupby([\n",
    "    \"symbol\", \"statement_type\", \"period\", \"period_detail\", \n",
    "    \"period_end_date\", \"report_type\", \"consolidation_type\"\n",
    "]).cumcount() + 1\n",
    "\n",
    "# Now you can see the combinations with their order\n",
    "result = fin_stat_meta_df[[\"symbol\", \"statement_type\", \"period\", \"period_detail\", \n",
    "                          \"period_end_date\", \"report_type\", \"consolidation_type\", \n",
    "                          \"combination_order\"]].value_counts()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e18ed4",
   "metadata": {},
   "source": [
    "## PTL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f55ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptl = fin_stat_meta_df[fin_stat_meta_df[\"symbol\"] == \"PTL\"]\n",
    "ptl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e762fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptl_counts = ptl[[ \"period_end_date\", \"report_type\", \"consolidation_type\"]].value_counts()\n",
    "duplicates = ptl_counts[ptl_counts > 1]\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc621a",
   "metadata": {},
   "source": [
    "## 138SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbfc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_138sl = fin_stat_meta_df[fin_stat_meta_df[\"symbol\"] == \"138SL\"]\n",
    "symbol_138sl\n",
    "\n",
    "symbol_138sl_counts = symbol_138sl[[ \"period_end_date\", \"report_type\", \"consolidation_type\"]].value_counts()\n",
    "duplicates = symbol_138sl_counts[symbol_138sl_counts > 1]\n",
    "print(duplicates)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
